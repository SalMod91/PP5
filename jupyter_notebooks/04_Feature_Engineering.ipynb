{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# Feature Engineering Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "- Engineer features for Classification, Regression and Cluster models\n",
        "\n",
        "\n",
        "## Inputs\n",
        "The inputs for this stage will include cleaned datasets that have been previously prepared:\n",
        "- **\"outputs/datasets/cleaned/train_set.csv\"**: The training set used for training the models.\n",
        "- **\"outputs/datasets/cleaned/test_set.csv\"**: The test set used for model validation and performance evaluation.\n",
        "\n",
        "## Outputs\n",
        "- Generate a list of the variables that will be engineered, describing the nature of the transformations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "metadata": {},
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "metadata": {},
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "metadata": {},
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Cleaned Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, we load the preprocessed training and testing datasets to prepare for feature engineering and model development.\n",
        "\n",
        "We'll confirm that the data is properly formatted and ready for further analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the path to the cleaned training dataset\n",
        "train_set_path = \"outputs/datasets/cleaned/train_set.csv\"\n",
        "\n",
        "# Load the training dataset from the specified path into a DataFrame\n",
        "train_set = pd.read_csv(train_set_path)\n",
        "\n",
        "# Display the first few rows of the training dataset to verify its structure and content\n",
        "train_set.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Define the path to the cleaned testing dataset\n",
        "test_set_path = \"outputs/datasets/cleaned/test_set.csv\"\n",
        "\n",
        "# Load the testing dataset from the specified path into a DataFrame\n",
        "test_set = pd.read_csv(test_set_path)\n",
        "\n",
        "# Display the first few rows of the testing dataset to verify its structure and content\n",
        "test_set.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now use Pandas Profiling to analyze the variables in our datasets and evaluate potential transformations for feature engineering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "pandas_report = ProfileReport(df=train_set, minimal=True)\n",
        "pandas_report.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we'll utilize a function from the Feature-engine lesson from Code Institute to assist us with the feature engineering process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "from feature_engine import transformation as vt\n",
        "from feature_engine.outliers import Winsorizer\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style=\"whitegrid\")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "def FeatureEngineeringAnalysis(df,analysis_type=None):\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  - used for quick feature engineering on numerical and categorical variables\n",
        "  to decide which transformation can better transform the distribution shape \n",
        "  - Once transformed, use a reporting tool, like pandas-profiling, to evaluate distributions\n",
        "\n",
        "  \"\"\"\n",
        "  check_missing_values(df)\n",
        "  allowed_types= ['numerical', 'ordinal_encoder',  'outlier_winsorizer']\n",
        "  check_user_entry_on_analysis_type(analysis_type, allowed_types)\n",
        "  list_column_transformers = define_list_column_transformers(analysis_type)\n",
        "  \n",
        "  \n",
        "  # Loop over each variable and engineer the data according to the analysis type\n",
        "  df_feat_eng = pd.DataFrame([])\n",
        "  for column in df.columns:\n",
        "    # create additional columns (column_method) to apply the methods\n",
        "    df_feat_eng = pd.concat([df_feat_eng, df[column]], axis=1)\n",
        "    for method in list_column_transformers:\n",
        "      df_feat_eng[f\"{column}_{method}\"] = df[column]\n",
        "      \n",
        "    # Apply transformers in respectives column_transformers\n",
        "    df_feat_eng, list_applied_transformers = apply_transformers(analysis_type, df_feat_eng, column)\n",
        "\n",
        "    # For each variable, assess how the transformations perform\n",
        "    transformer_evaluation(column, list_applied_transformers, analysis_type, df_feat_eng)\n",
        "\n",
        "  return df_feat_eng\n",
        "\n",
        "\n",
        "def check_user_entry_on_analysis_type(analysis_type, allowed_types):\n",
        "  ### Check analyis type\n",
        "  if analysis_type == None:\n",
        "    raise SystemExit(f\"You should pass analysis_type parameter as one of the following options: {allowed_types}\")\n",
        "  if analysis_type not in allowed_types:\n",
        "      raise SystemExit(f\"analysis_type argument should be one of these options: {allowed_types}\")\n",
        "\n",
        "def check_missing_values(df):\n",
        "  if df.isna().sum().sum() != 0:\n",
        "    raise SystemExit(\n",
        "        f\"There is missing values in your dataset. Please handle that before getting into feature engineering.\")\n",
        "\n",
        "\n",
        "\n",
        "def define_list_column_transformers(analysis_type):\n",
        "  ### Set suffix colummns acording to analysis_type\n",
        "  if analysis_type=='numerical':\n",
        "    list_column_transformers = [\"log_e\",\"log_10\",\"reciprocal\", \"power\",\"box_cox\",\"yeo_johnson\"]\n",
        "  \n",
        "  elif analysis_type=='ordinal_encoder':\n",
        "    list_column_transformers = [\"ordinal_encoder\"]\n",
        "\n",
        "  elif analysis_type=='outlier_winsorizer':\n",
        "    list_column_transformers = ['iqr']\n",
        "\n",
        "  return list_column_transformers\n",
        "\n",
        "\n",
        "\n",
        "def apply_transformers(analysis_type, df_feat_eng, column):\n",
        "\n",
        "\n",
        "  for col in df_feat_eng.select_dtypes(include='category').columns:\n",
        "    df_feat_eng[col] = df_feat_eng[col].astype('object')\n",
        "\n",
        "\n",
        "  if analysis_type=='numerical':\n",
        "    df_feat_eng,list_applied_transformers = FeatEngineering_Numerical(df_feat_eng,column)\n",
        "  \n",
        "  elif analysis_type=='outlier_winsorizer':\n",
        "    df_feat_eng,list_applied_transformers = FeatEngineering_OutlierWinsorizer(df_feat_eng,column)\n",
        "\n",
        "  elif analysis_type=='ordinal_encoder':\n",
        "    df_feat_eng,list_applied_transformers = FeatEngineering_CategoricalEncoder(df_feat_eng,column)\n",
        "\n",
        "  return df_feat_eng,list_applied_transformers\n",
        "\n",
        "\n",
        "\n",
        "def transformer_evaluation(column, list_applied_transformers, analysis_type, df_feat_eng):\n",
        "  # For each variable, assess how the transformations perform\n",
        "  print(f\"* Variable Analyzed: {column}\")\n",
        "  print(f\"* Applied transformation: {list_applied_transformers} \\n\")\n",
        "  for col in [column] + list_applied_transformers:\n",
        "    \n",
        "    if analysis_type!='ordinal_encoder':\n",
        "      DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "    \n",
        "    else:\n",
        "      if col == column: \n",
        "        DiagnosticPlots_Categories(df_feat_eng, col)\n",
        "      else:\n",
        "        DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Categories(df_feat_eng, col):\n",
        "  plt.figure(figsize=(10, 2.5))\n",
        "  sns.countplot(data=df_feat_eng, x=col,palette=['#432371'],order = df_feat_eng[col].value_counts().index)\n",
        "  plt.xticks(rotation=90) \n",
        "  plt.suptitle(f\"{col}\", fontsize=30,y=1.05)        \n",
        "  plt.show();\n",
        "  print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Numerical(df, variable):\n",
        "  fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "  sns.histplot(data=df, x=variable, kde=True,element=\"step\",ax=axes[0]) \n",
        "  stats.probplot(df[variable], dist=\"norm\", plot=axes[1])\n",
        "  sns.boxplot(x=df[variable],ax=axes[2])\n",
        "  \n",
        "  axes[0].set_title('Histogram')\n",
        "  axes[1].set_title('QQ Plot')\n",
        "  axes[2].set_title('Boxplot')\n",
        "  fig.suptitle(f\"{variable}\", fontsize=30,y=1.05)\n",
        "  plt.tight_layout()\n",
        "  plt.show();\n",
        "\n",
        "\n",
        "def FeatEngineering_CategoricalEncoder(df_feat_eng,column):\n",
        "  list_methods_worked = []\n",
        "  try:  \n",
        "    encoder= OrdinalEncoder(encoding_method='arbitrary', variables = [f\"{column}_ordinal_encoder\"])\n",
        "    df_feat_eng = encoder.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_ordinal_encoder\")\n",
        "  \n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_ordinal_encoder\"],axis=1,inplace=True)\n",
        "    \n",
        "  return df_feat_eng,list_methods_worked\n",
        "\n",
        "\n",
        "def FeatEngineering_OutlierWinsorizer(df_feat_eng,column):\n",
        "  list_methods_worked = []\n",
        "\n",
        "  ### Winsorizer iqr\n",
        "  try: \n",
        "    disc=Winsorizer(\n",
        "        capping_method='iqr', tail='both', fold=1.5, variables = [f\"{column}_iqr\"])\n",
        "    df_feat_eng = disc.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_iqr\")\n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_iqr\"],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  return df_feat_eng,list_methods_worked\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def FeatEngineering_Numerical(df_feat_eng,column):\n",
        "\n",
        "  list_methods_worked = []\n",
        "\n",
        "  ### LogTransformer base e\n",
        "  try: \n",
        "    lt = vt.LogTransformer(variables = [f\"{column}_log_e\"])\n",
        "    df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_log_e\")\n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_log_e\"],axis=1,inplace=True)\n",
        "\n",
        "    ### LogTransformer base 10\n",
        "  try: \n",
        "    lt = vt.LogTransformer(variables = [f\"{column}_log_10\"],base='10')\n",
        "    df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_log_10\")\n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_log_10\"],axis=1,inplace=True)\n",
        "\n",
        "  ### ReciprocalTransformer\n",
        "  try:\n",
        "    rt = vt.ReciprocalTransformer(variables = [f\"{column}_reciprocal\"])\n",
        "    df_feat_eng =  rt.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_reciprocal\")\n",
        "  except:\n",
        "    df_feat_eng.drop([f\"{column}_reciprocal\"],axis=1,inplace=True)\n",
        "\n",
        "  ### PowerTransformer\n",
        "  try:\n",
        "    pt = vt.PowerTransformer(variables = [f\"{column}_power\"])\n",
        "    df_feat_eng = pt.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_power\")\n",
        "  except:\n",
        "    df_feat_eng.drop([f\"{column}_power\"],axis=1,inplace=True)\n",
        "\n",
        "  ### BoxCoxTransformer\n",
        "  try:\n",
        "    bct = vt.BoxCoxTransformer(variables = [f\"{column}_box_cox\"])\n",
        "    df_feat_eng = bct.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_box_cox\")\n",
        "  except:\n",
        "    df_feat_eng.drop([f\"{column}_box_cox\"],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  ### YeoJohnsonTransformer\n",
        "  try:\n",
        "    yjt = vt.YeoJohnsonTransformer(variables = [f\"{column}_yeo_johnson\"])\n",
        "    df_feat_eng = yjt.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_yeo_johnson\")\n",
        "  except:\n",
        "        df_feat_eng.drop([f\"{column}_yeo_johnson\"],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  return df_feat_eng,list_methods_worked"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "During the feature engineering phase, we focus exclusively on the training dataset to prepare our data for the subsequent modeling stages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use the following transformers to enhance the predictive power of our dataset:\n",
        "- **Categorical Encoding**: Convert text variables into numerical format to facilitate their use in machine learning models.\n",
        "- **Numerical Transformations**: Apply transformations to numerical variables to improve their distribution, which is expected to enhance model performance.\n",
        "- **Smart Correlation Selection**: Evaluate all variables for redundancy using correlation analysis and remove any that are deemed unnecessary, streamlining the feature set for optimal efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These steps are designed to prepare our data, ensuring it is optimized for the modeling process that will follow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Categorical Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will convert text variables into numerical format in the following steps:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 1**: Identify categorical variables in the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Selecting categorical variables from the train_set DataFrame\n",
        "# 'select_dtypes' is used to filter columns based on data type\n",
        "# Including both 'object' and 'category' data types to capture all possible categorical columns\n",
        "categorical_variables = list(train_set.select_dtypes(['object','category']).columns)\n",
        "\n",
        "# Display the list of categorical variables\n",
        "categorical_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 2**: Create a new DataFrame containing the previously identified categorical variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Create a new DataFrame by extracting the columns listed in 'categorical_variables' from 'train_set'\n",
        "# The 'copy()' function is used to ensure that the new DataFrame is a separate object\n",
        "df_categorical_engineering = train_set[categorical_variables].copy()\n",
        "\n",
        "# Display the first few rows of the new DataFrame\n",
        "df_categorical_engineering.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 3**: Transform the variables and evaluate their distributions to determine the most appropriate method for each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Apply the 'FeatureEngineeringAnalysis' class to the DataFrame\n",
        "# Specify to perform an 'ordinal_encoder' analysis, converting categorical text data into a numerical format\n",
        "categorical_transformed = FeatureEngineeringAnalysis(df=df_categorical_engineering, analysis_type='ordinal_encoder')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 4**: Implement the transformation on both the Train and Test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Set the encoding method to 'arbitrary' which assigns integers to the categories in the order they are observed.\n",
        "# Specify the variables to be encoded using the list 'categorical_variables'.\n",
        "encoder = OrdinalEncoder(encoding_method='arbitrary', variables=categorical_variables)\n",
        "\n",
        "# Applies the transformation on the training set\n",
        "train_set = encoder.fit_transform(train_set)\n",
        "\n",
        "# Applies the transformation on the testing set\n",
        "test_set = encoder.transform(test_set)\n",
        "\n",
        "# Print a confirmation message\n",
        "print(\"* Categorical encoding - ordinal transformation done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Numerical Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, we will apply transformations to numerical variables to optimize their distributions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 1**: Identify numerical variables in the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Variables to be excluded\n",
        "exclude_vars = ['BsmtExposure', 'BsmtFinType1', 'GarageFinish', 'KitchenQual', 'SalePrice']\n",
        "\n",
        "# Extract a list of numerical variables from the train_set DataFrame\n",
        "# 'select_dtypes' is used to filter columns based on data type, including both integer and float types\n",
        "all_numerical_variables = list(train_set.select_dtypes(include=['int64', 'float64']).columns)\n",
        "\n",
        "# Exclude the transformed categorical variables from the list\n",
        "numerical_variables = [var for var in all_numerical_variables if var not in exclude_vars]\n",
        "\n",
        "# Display the list of numerical variables\n",
        "numerical_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 2**: Create a new DataFrame containing the previously identified numerical variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Create a new DataFrame by extracting the columns listed in 'numerical_variables' from 'train_set'\n",
        "# The 'copy()' function is used to ensure that the new DataFrame is a separate object\n",
        "df_numerical_engineering = train_set[numerical_variables].copy()\n",
        "\n",
        "# Display the first few rows of the new DataFrame\n",
        "df_numerical_engineering.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 3**: Transform the variables and evaluate their distributions to determine the most appropriate method for each and implement the transformation on both the Train and Test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Apply the 'FeatureEngineeringAnalysis' class to the DataFrame\n",
        "# The parameter 'analysis_type' is set to 'numerical', indicating that the function should perform numerical transformations.\n",
        "numerical_transformed = FeatureEngineeringAnalysis(df=df_numerical_engineering, analysis_type='numerical')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Feature          | Transformation Decision                                   | Reasoning                                                                 |\n",
        "|------------------|-----------------------------------------------------------|---------------------------------------------------------------------------|\n",
        "| **1stFlrSF**     | <span style=\"color:yellow\">Log</span>                     | Log improves distribution and QQ plot alignment significantly.            |\n",
        "| **2ndFlrSF**     | None                                                      | Minimal impact from power; no substantial justification for change.       |\n",
        "| **BedroomAbvGr** | None                                                      | No significant improvement from any transformation.                       |\n",
        "| **BsmtFinType1** | <span style=\"color:red\">Power</span>                      | Power improves distribution and QQ plot, especially excluding outlier (0).|\n",
        "| **BsmtUnfSF**    | <span style=\"color:red\">Power</span>                      | Both Power and Yeo-Johnson improve distribution and QQ plots comparably.  |\n",
        "| **GarageArea**   | <span style=\"color:green\">Yeo-Johnson</span>              | Best improvement in QQ plots, especially excluding outlier (0).           |\n",
        "| **GrLivArea**    | <span style=\"color:yellow\">Log</span>                     | Significantly enhances distribution and QQ plot.                          |\n",
        "| **LotArea**      | <span style=\"color:red\">Power</span>                      | Drastically improves QQ plot alignment.                                   |\n",
        "| **LotFrontage**  | None                                                      | Better original QQ plot despite improved histogram with Power and Yeo.    |\n",
        "| **MasVnrArea**   | <span style=\"color:red\">Power</span>                      | Significant improvement in both distribution and QQ plot excluding (0).   |\n",
        "| **OpenPorchSF**  | <span style=\"color:red\">Power</span>                      | Notable improvement in QQ plot over Yeo-Johnson.                          |\n",
        "| **OverallCond**  | None                                                      | No transformation shows significant improvement.                          |\n",
        "| **OverallQual**  | None                                                      | No significant improvement from any transformation.                       |\n",
        "| **TotalBsmtSF**  | <span style=\"color:red\">Power</span>                      | Leads in QQ plot improvement over Yeo-Johnson.                            |\n",
        "| **YearBuilt**    | None                                                      | No transformation yields significant improvements.                        |\n",
        "| **YearRemodAdd** | None                                                      | No significant improvement from any transformation.                       |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Define the variables to be transformed using Log, Power, and Yeo-Johnson transformations\n",
        "variables_log = [\"1stFlrSF\", \"GrLivArea\"]\n",
        "variables_power = [\"BsmtFinType1\", \"BsmtUnfSF\", \"LotArea\", \"MasVnrArea\", \"OpenPorchSF\", \"TotalBsmtSF\"]\n",
        "variables_yeo = [\"GarageArea\"]\n",
        "\n",
        "# Create a pipeline comprising the specified transformations for specified groups of variables\n",
        "pipeline = Pipeline([\n",
        "      (\"log\", vt.LogTransformer(variables = variables_log, base= 'e')),\n",
        "      (\"pwr\", vt.PowerTransformer(variables = variables_power)),\n",
        "      (\"yeo\", vt.YeoJohnsonTransformer(variables = variables_yeo))\n",
        "    ])\n",
        "\n",
        "# Fit the pipeline to the training set and transform the training data\n",
        "train_set = pipeline.fit_transform(train_set)\n",
        "\n",
        "# Apply the same transformations to the test set\n",
        "test_set = pipeline.transform(test_set)\n",
        "\n",
        "# Print confirmation that the numerical transformations have been completed\n",
        "print(\"* The numerical transformation has been completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will verify that the transformations have been accurately applied to both the training and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "train_set.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "test_set.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Smart Correlated Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, we will use smart correlated selection across all features to eliminate redundant ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 1**: Identify the variables for smart correlated selection, focusing exclusively on the features.\n",
        "\n",
        "- In this analysis, we exclude the target variable because our objective is to examine correlations among features only.\n",
        "\n",
        "- The primary goal is to reduce redundancy within the predictors in our model. Since we aim to model the relationship between features and the target, removing the target due to its correlation with features would counteract the purpose of predictive modeling.\n",
        "\n",
        "In summary, we keep **Sale Price** out of the feature selection process that aims to eliminate correlated predictors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "df_correlation_variables = train_set.drop('SalePrice', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 2**: Create a new DataFrame containing the previously identified variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Create a new DataFrame by extracting the columns listed in 'correlation_variables' from 'train_set'\n",
        "# The 'copy()' function is used to ensure that the new DataFrame is a separate object\n",
        "df_correlation_engineering = df_correlation_variables.copy()\n",
        "\n",
        "# Display the first few rows of the new DataFrame\n",
        "df_correlation_engineering.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 3**: Analyze the correlation among features and identify highly correlated pairs to determine which variables to remove for reducing redundancy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "\n",
        "# Initialize the SmartCorrelatedSelection transformer. \n",
        "# \"variables=None\" considers all numerical variables in the DataFrame.\n",
        "# \"method=\"spearman\" specifies using the Spearman rank correlation.\n",
        "# \"threshold=0.8\" sets the minimum correlation coefficient for which features will be considered highly correlated.\n",
        "# \"selection_method=\"variance\" indicates that among correlated groups, the feature with the lowest variance is dropped.\n",
        "corr_sel = SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.8, selection_method=\"variance\")\n",
        "\n",
        "corr_sel.fit_transform(df_correlation_engineering)\n",
        "corr_sel.correlated_feature_sets_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 4**: We will remove any excess among the correlated features since they contribute redundant information to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Retrieve and display the list of features that were identified as redundant\n",
        "corr_sel.features_to_drop_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Conclusions and Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feature Engineering Transformations:\n",
        "- **Ordinal Categorical Encoding**: Applied to variables such as **BsmtExposure**, **BsmtFinType1**, **GarageFinish**, and **KitchenQual**.\n",
        "- **Log-e Numerical Transformation**: Implemented on **GrLivArea** to enhance its distribution.\n",
        "- **Power Numerical Transformation**: Used for **BsmtUnfSF**, **BsmtFinType1**, **MasVnrArea**, **OpenPorchSF**, and **TotalBsmtSF** to adjust their scales and distributions.\n",
        "- **Yeo-Johnson Numerical Transformation**: Applied to **GarageArea** for normalization.\n",
        "- **Smart Correlated Selection**: **1stFlrSF** was identified as redundant and removed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "- **Modeling**: Proceed with selecting and applying a linear regression algorithm to the refined dataset.\n",
        "- **Hyperparameter Tuning**: Optimize the modelâ€™s settings to achieve the best performance possible."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
